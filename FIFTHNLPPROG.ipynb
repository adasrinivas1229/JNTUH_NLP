{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwLH7qzgIP4fM9sN2tLRSt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adasrinivas1229/JNTUH_NLP/blob/main/FIFTHNLPPROG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Here's a simple Python program that implements N-gram smoothing in NLP:"
      ],
      "metadata": {
        "id": "bwlqkQ5mFnFp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xC9JV4ccD-y6",
        "outputId": "91b52f36-e067-4b43-81b7-f3f99fcb4342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilities of bigrams:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.lm import KneserNeyInterpolated\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample text\n",
        "text = \"Natural language processing (NLP) is the ability of a computer program to understand human language as it is spoken.\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Generate bigrams from the tokens\n",
        "bigrams = ngrams(tokens, 2)\n",
        "\n",
        "# Train a Kneser-Ney Interpolated (KN-Interpolated) language model on the bigrams\n",
        "model = KneserNeyInterpolated(2)\n",
        "model.fit(bigrams,bigrams)\n",
        "\n",
        "# Evaluate the model's probabilities of bigrams\n",
        "print(\"Probabilities of bigrams:\")\n",
        "for bigram in bigrams:\n",
        "    prob = model.score(bigram)\n",
        "    print(\"{}: {:.3f}\".format(bigram, prob))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This program uses the word_tokenize function from the nltk module to split the text into words. It then generates bigrams (pairs of words) from the tokens using the ngrams function. A Kneser-Ney Interpolated (KN-Interpolated) language model is trained on the bigrams using the KneserNeyInterpolated class from the nltk.lm module. The model is then used to evaluate the probabilities of the bigrams in the text.\n",
        "\n",
        "Kneser-Ney smoothing is a type of N-gram smoothing that adjusts the probabilities of N-grams based on the frequency of lower-order N-grams in the training data. This helps to avoid the issue of zero probability N-grams that can arise with simple maximum likelihood estimation."
      ],
      "metadata": {
        "id": "5ijMnt4sHCMB"
      }
    }
  ]
}