{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxAh2isQ7gYLMGDtUi3ZuD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adasrinivas1229/JNTUH_NLP/blob/main/FIRSTNLPPROG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Here's a simple Python program that performs basic word analysis in NLP:"
      ],
      "metadata": {
        "id": "DdH073QKqXwH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "431ii7ugqL5j",
        "outputId": "2b015be4-8996-4b9d-da30-f526b9a7cfaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 10 most frequent words are:\n",
            "language: 2\n",
            "is: 2\n",
            "Natural: 1\n",
            "processing: 1\n",
            "(: 1\n",
            "NLP: 1\n",
            "): 1\n",
            "the: 1\n",
            "ability: 1\n",
            "of: 1\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "  \n",
        "\n",
        "# Sample text\n",
        "text = \"Natural language processing (NLP) is the ability of a computer program to understand human language as it is spoken.\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Perform frequency analysis on the tokens\n",
        "word_freq = nltk.FreqDist(tokens)\n",
        "\n",
        "# Print the 10 most frequent words\n",
        "print(\"The 10 most frequent words are:\")\n",
        "for word, freq in word_freq.most_common(10):\n",
        "    print(f\"{word}: {freq}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This program uses the word_tokenize function from the nltk.tokenize module to split the text into words. It then uses the nltk.FreqDist class to perform frequency analysis on the tokens and finds the 10 most frequent words. The frequency of each word is then printed."
      ],
      "metadata": {
        "id": "pWF3ZGGtrWue"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AQtsjoVSrcqG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}